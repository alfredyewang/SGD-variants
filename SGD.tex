\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}}
\newcommand{\Q}{\bb{Q}}
\newcommand{\R}{\bb{R}}
\newcommand{\C}{\bb{C}}
\newcommand{\N}{\bb{N}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}}
\newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;}
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}

%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}



\begin{document}


\begin{center}
{\Large Reviews of stochastic gradient descent variants}\\
\end{center}

\vspace{0.2 cm}


% \subsection*{Exercises for Section 1.1: Norm and Inner Product} star means no numbers
\subsection*{Introduction}
We firstly give up the least square method of linear regression, and then address linear regression problem based on a general machine learning method. In my opinion, Machine learning algorithm consists of \textbf{model, strategy} and \textbf{optimization}.\\
\\
\centerline{Learning algorithm = Model + Strategy + Optimization;}\\

The first thing we should do is to determine what decision function or conditional probability distribution we need to learn and get. For instance, in linear regression we want to learn $h_\theta(x)=\theta x$. This decision function is the model we want to use.

We have the model, but we do not know parameters in it, and next thing we should do is parameter estimation, which means how to construct loss function, and we call this step strategy.

The last step is optimization which means how to solve the loss function.
\subsection*{Notation}
\subsection*{Gradient Descent}
\subsection*{Stoastic Gradient Descent}
\subsection*{Stoastic Average Gradient Descent(SAG)}
\subsection*{Stoastic Gradient Descent with Predictive Variance Reduction(SVRG)}
% \bibliographystyle{abbrv}
% \bibliography{mylib}
%
%
% \begin{thebibliography}{9}
% \bibitem{latexcompanion}
% Michel Goossens, Frank Mittelbach, and Alexander Samarin.
% \textit{The \LaTeX\ Companion}.
% Addison-Wesley, Reading, Massachusetts, 1993.
%
% \bibitem{einstein}
% Albert Einstein.
% \textit{Zur Elektrodynamik bewegter K{\"o}rper}. (German)
% [\textit{On the electrodynamics of moving bodies}].
% Annalen der Physik, 322(10):891â€“921, 1905.
%
% \bibitem{knuthwebsite}
% Knuth: Computers and Typesetting,
% \\\texttt{http://www-cs-faculty.stanford.edu/\~{}uno/abcde.html}
% \end{thebibliography}



\end{document}
